
# 感知机

- 给定输入 $x$ ,权重，偏移，感知机输出
- 二分类：-1或者1
	- Vs.softmax
	- Vs.线性回归
- 训练感知机 （林轩田）
	- 等价于使用批量大小为1的梯度下降
- 收敛定理
	- 数据在半径r内
	- 余量
	- 保证收敛
	- 线性可分
- $XOR$ 问题
	- 感知机不能拟合 $XOR$ 问题，只能产生线性分割面


- 感知机是一个二分类模型，是最早的AI模型之一
- 它的求解算法等价于使用批量大小为1的梯度下降
- 不能拟合XOR，导致了第一次AI寒冬

# 多次感知机

- 学习$XOR$
- 单隐藏层
	- Input layer
	- Hidden layer
		- 大小是一个超参数
	- Output layer
	- 单分类
- 激活函数
	- 为什么需要非线性的激活函数
		- 算出来的还是线性模型
	- Sigmoid激活函数
		- 将输入投影到$(0,1)$,是一个软的 soft的
	- Tanh激活函数
		- 将输入投影到$(-1,1)$
	- ReLU激活函数
		- ReLU: rectified linear unit
		- 算起来很快，不用做指数运算
- 多类分类
	- $softmax(o_1,o_2,\dots,o_k)$
- 多隐藏层
	- 超参数
		- 隐藏层数
		- 每层隐藏层的大小
		- 输入数据比较难
			- 单隐藏层m比较大
			- 模型变深，m1小一点，m2小一点，m3更小

# 代码实现


# QA

- 一层网络具体是指什么
	- 通常代指带权重的一层
	- 箭头 可以学习的权重
	- 激活函数不是单独一层
- SVM 替代mlp
	- SVM和MLP差不多，SVM的数学性跟好
- 机械学习 等于 统计学的计算机领域
- 增加隐藏层的层数 ，而不是增加神经元个数
	- 模型复杂度几乎是等价的
	- 太胖的不好训练
	- 太胖的容易过拟合
	- 并行的东西调整起来很难
- 神经元和卷积核的关系
- 