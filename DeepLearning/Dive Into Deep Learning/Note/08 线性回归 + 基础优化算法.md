
# 线性回归

- 如何在美国买房
	- 预期价格
	- 房价预测
- 一个简化模型
	- 卧室个数，卫生间个数，居住面积 
	- 成交价是关键因素的加权和
- 线性模型
	- 给定n维输入$\mathbf{x} = [x_1,x_2,\ldots,x_n]^T$
	- 线性模型有一个n维权重和一个标量偏差
	- 向量版本
- 线性模型可以看作是单纯神经网络
	- Output layer
	- Input layer
- 神经网络源于神经科学
	- 输入
	- 计算
	- 输出到下一层
- 衡量预测质量
	- 平方损失
- 训练数据
	- 决定参数值
	- 通常越多越好
- 参数学习
	- 训练损失
	- 最小化损失来学习参数
- 显式解
	- 有最优解

# 基础优化方法

- 梯度下降
	- 挑选一个参数的随机初始值$W_0$
	- 重复迭代参数t = 1,2,3
		- 学习率：步长的超参数
		- 选择**学习率**
			- 不能太小，不能太大（震荡 锯齿现象）
	- 小批量随机梯度下降
		- 在整个训练集上算梯度太贵
		- 可以随机采样b个样本来近似损失
		- b是**批量大小**，是另外一个重要的超参数
			- 选择合适的批量大小

# 线性回归的从零开始实现

# 线性回归的简洁实现


# QA

- Colab的问题 对学生党便宜的平台
	- Colab，aws
- 平方损失，绝对插值（不可导
- 损失为什么要求平均？
	- 没啥问题
	- 改学习率就好了，调学习率比较好调
- 线性回归损失函数通常都是mse
- 神经网络，神经元没有误差反馈
	- 神经元会根据负反馈调整
	- 感知机
- 如何调整学习率
	- gd梯度下降 sgd随机梯度下降
	- 预训练模型
- batchsize 是否会影响最终模型的结果
	- 小没问题，大有问题
	- 噪音是好事情
	- 防止过拟合
- 过拟合，和欠拟合情况下 ，如何调整学习率和批量大小
- batchsize中每个样本对应参数求和得到梯度的平均值
- 随机梯度下降=随机=随机采样
- 正则问题
- 为什么都算梯度下降（一阶导），二阶导太难算了，**牛顿法**
	- 模型不真实，求模型的最优解没啥用
	- 收敛块没啥用，对泛化性没帮助
	- 还需要再发展
- detach()是干嘛的
	- 从计算图中去除
- data-iter每次把所有数据都load进去，内存会爆炸
	- 不要太担心这个问题
	- 实际情况中，数据丢在硬盘上
- 样品大小不是批量书的整数
	- 补全 ，丢弃，直接用
- 衰减，减小学习率，先不讲
- 收敛的判断
- 实际中网络中有nan，求导有除法